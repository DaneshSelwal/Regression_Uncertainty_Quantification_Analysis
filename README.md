# ğŸš€ Comprehensive Regression Analysis Pipeline: Advanced Uncertainty Quantification

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-XGBoost%20%7C%20LightGBM%20%7C%20CatBoost-orange?style=for-the-badge)
![Uncertainty Quantification](https://img.shields.io/badge/Uncertainty-Adaptive%20CP%20%7C%20NEXCP%20%7C%20Quantile-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

Welcome to the **End-to-End Regression Analysis Pipeline**. This repository is engineered as a modular, "plug-and-play" framework for robust regression tasks. It goes beyond simple point predictions by integrating a suite of **Uncertainty Quantification (UQ)** methods, ensuring that every prediction is accompanied by a reliable confidence interval.

Whether you are analyzing environmental data, financial time-series, or industrial sensor readings, this pipeline allows you to swap in your dataset and immediately leverage state-of-the-art Hyperparameter Tuning, Quantile Regression, Probabilistic Modeling, and Adaptive Conformal Prediction.

---

## ğŸ“‘ Table of Contents (Navigation)

1. [ğŸ“Œ Project Overview](#-project-overview)
2. [ğŸ“‚ Repository Structure](#-repository-structure)
3. [ğŸ“Š Dataset & Usage](#-dataset--usage)
4. [ğŸ› ï¸ Workflow & Methodology](#-workflow--methodology)
    - [Phase 1: Hyperparameter Tuning](#phase-1-hyperparameter-tuning)
    - [Phase 2: Quantile Regression](#phase-2-quantile-regression)
    - [Phase 3: Probabilistic Distribution](#phase-3-probabilistic-distribution)
    - [Phase 3b: Probabilistic Distribution (CARD)](#phase-3b-probabilistic-distribution-card)
    - [Phase 4: Standard Conformal Predictions](#phase-4-standard-conformal-predictions)
    - [Phase 5: Adaptive & Non-Exchangeable CP](#phase-5-adaptive--non-exchangeable-cp)
5. [ğŸš€ Getting Started](#-getting-started)

---

## ğŸ“Œ Project Overview

This framework provides a rigorous path from raw data to confident predictions. It is designed to be **domain-agnostic**: while the inspiration comes from hydrological sediment load analysis, the methods are applicable to any regression problem, especially those involving time-series or non-exchangeable data.

**Key Features:**
*   **Automated Optimization**: Harnessing **Optuna** for Bayesian optimization of complex regressors.
*   **Interval Estimation**: **Quantile Regression** for estimating conditional bounds (e.g., 5th and 95th percentiles).
*   **Full Distribution Modeling**: Using **NGBoost** and **PGBM** to predict the full probability distribution parameters ($\mu, \sigma$).
*   **Generative Modeling**: Leveraging **CARD (Classification and Regression Diffusion)** models to generate conditional distributions using diffusion processes.
*   **Robust Uncertainty**: Implementation of **NEXCP (Non-Exchangeable Conformal Prediction)** and **Adaptive CP**, crucial for handling data drift and temporal dependencies where standard methods fail.

---

## ğŸ“‚ Repository Structure

The project is encapsulated within the `Data_folder`, organized by analysis phase.

```
.
â”œâ”€â”€ Data_folder/
â”‚   â”œâ”€â”€ Data/                                       # ğŸ“ Input Data (Entry Point)
â”‚   â”‚   â”œâ”€â”€ train.csv                               # Training dataset
â”‚   â”‚   â””â”€â”€ test.csv                                # Testing dataset
â”‚   â”‚
â”‚   â”œâ”€â”€ HyperParameter_Tuning/                      # ğŸ›ï¸ Phase 1: Optimization
â”‚   â”‚   â”œâ”€â”€ Optuna_autosampler.ipynb                # Optuna Bayesian Optimization script
â”‚   â”‚   â””â”€â”€ models/                                 # Saved optimized models
â”‚   â”‚
â”‚   â”œâ”€â”€ Quantile_Regression/                        # ğŸ“‰ Phase 2: Quantile Methods
â”‚   â”‚   â”œâ”€â”€ Quantile_Regression.ipynb               # Script for Quantile Regression
â”‚   â”‚   â””â”€â”€ Results/                                # Prediction outputs
â”‚   â”‚
â”‚   â”œâ”€â”€ Probabilistic_Distribution/                 # ğŸ“Š Phase 3: Distributional Models
â”‚   â”‚   â”œâ”€â”€ Probabilistic__Distribution.ipynb       # NGBoost & PGBM implementation
â”‚   â”‚   â””â”€â”€ Results/                                # Calibration plots & CRPS scores
â”‚   â”‚
â”‚   â”œâ”€â”€ Probabilistic_Distribution(CARD)/           # ğŸŒ«ï¸ Phase 3b: Diffusion Models (CARD)
â”‚   â”‚   â””â”€â”€ Probabilistic__Distribution(CARD).ipynb # Diffusion-based distribution modeling
â”‚   â”‚
â”‚   â”œâ”€â”€ Conformal_Predictions(MAPIE,PUNCC)/         # ğŸ›¡ï¸ Phase 4: Standard CP
â”‚   â”‚   â””â”€â”€ Conformal Predictions(MAPIE,PUNCC).ipynb
â”‚   â”‚
â”‚   â””â”€â”€ Conformal_Predictions(NEXCP,AdaptiveCP,mfcs)/ # ğŸ›¡ï¸ Phase 5: Advanced Time-Series CP
â”‚       â””â”€â”€ Conformal_Predictions(NEXCP, Adaptive CP, mfcs).ipynb
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset & Usage

**This is a Template Pipeline.**

To use this repository with your own data:

1.  **Prepare your data**: You need a training set and a testing set.
2.  **Format**: Ensure your files are in `.csv` format.
3.  **Replace**:
    *   Replace `Data_folder/Data/train.csv` with your training data.
    *   Replace `Data_folder/Data/test.csv` with your testing data.
4.  **Configure**:
    *   **Column Names**: Open the notebooks (e.g., `Optuna_autosampler.ipynb`) and ensure the column names match your dataset's target variable and features.
    *   **File Paths**: Some notebooks may contain hardcoded paths (e.g., `/content/drive/MyDrive/...`) from the original Google Colab environment. You must update these paths to point to your local `Data_folder` location.

The default configuration assumes a structure with predictor columns and a target column. Adjust the "Target" variable name in the scripts to match your specific regression problem.

---

## ğŸ› ï¸ Workflow & Methodology

### Phase 1: Hyperparameter Tuning
**Location**: `Data_folder/HyperParameter_Tuning`
Before any uncertainty quantification, we must ensure our base estimators are accurate.
*   **Tool**: **Optuna**.
*   **Process**: We search over hyperparameter spaces for XGBoost, CatBoost, LightGBM, etc., using efficient pruners (Hyperband) to find the best configuration.
*   **Output**: Optimized model parameters saved for subsequent steps.

### Phase 2: Quantile Regression
**Location**: `Data_folder/Quantile_Regression`
We move beyond the mean.
*   **Goal**: Predict conditional quantiles (e.g., $Q_{0.05}$ and $Q_{0.95}$) to bracket the target value.
*   **Loss Function**: Pinball Loss.
*   **Result**: A prediction interval that captures a specified percentage of the data (e.g., 90%).

### Phase 3: Probabilistic Distribution
**Location**: `Data_folder/Probabilistic_Distribution`
Treating the target as a random variable $Y|X \sim \mathcal{D}(\theta)$.
*   **Models**: **NGBoost** (Natural Gradient Boosting) and **PGBM** (Probabilistic Gradient Boosting Machines).
*   **Metrics**: Negative Log-Likelihood (NLL) and Continuous Ranked Probability Score (CRPS).
*   **Visualization**: Probability Integral Transform (PIT) histograms to verify calibration.

### Phase 3b: Probabilistic Distribution (CARD)
**Location**: `Data_folder/Probabilistic_Distribution(CARD)`
Using generative diffusion models to capture complex conditional distributions.
*   **Models**: **CARD** (Classification and Regression Diffusion).
*   **Method**: Converts the regression target into a noise distribution and learns to reverse the diffusion process conditioned on features.
*   **Advantage**: Capable of modeling multi-modal distributions and complex dependencies.

### Phase 4: Standard Conformal Predictions
**Location**: `Data_folder/Conformal_Predictions(MAPIE,PUNCC)`
For data that satisfies the **exchangeability** assumption (i.e., order doesn't matter).
*   **Libraries**: `MAPIE`, `PUNCC`.
*   **Methods**: Split Conformal, CV+, Jackknife+.
*   **Guarantee**: Provides marginal coverage guarantees with finite-sample validity.

### Phase 5: Adaptive & Non-Exchangeable CP
**Location**: `Data_folder/Conformal_Predictions(NEXCP,AdaptiveCP,mfcs)`
**Crucial for Time-Series**.
Real-world data often drifts or has temporal dependencies.
*   **NEXCP**: Non-Exchangeable Conformal Prediction. Weights recent observations more heavily to adapt to distribution shifts.
*   **Adaptive CP**: Dynamically updates the interval width $C_t$ based on recent coverage errors.
*   **Result**: Valid coverage even during volatile periods (e.g., market crashes, floods).

---

## ğŸš€ Getting Started

1.  **Clone the Repository**:
    ```bash
    git clone <repo_url>
    cd <repo_directory>
    ```

2.  **Install Dependencies**:
    Ensure you have Python 3.10+ and the required libraries:
    ```bash
    pip install optuna xgboost lightgbm catboost ngboost pgbm mapie puncc
    ```
    *(Note: Check individual notebooks for specific library versions)*

3.  **Run the Pipeline**:
    Execute the notebooks in the order presented in the **Repository Structure** (Hyperparameter Tuning $\rightarrow$ Quantile/Probabilistic $\rightarrow$ Conformal Predictions).


     <sup>**</sup>This repository is a collaborative project developed under guidance of Dr. Mahesh Pal by Prakriti Bisht and Danesh Selwal.
---
